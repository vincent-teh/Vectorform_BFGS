# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10kt0UjGKhpPty-S4yIs6oIpjayD8AGJQ
"""

import sys

import torch
import torch.nn as nn
import torch.nn.functional as Functional
import torch.utils
import torchvision
import torchvision.transforms as Transforms
import tqdm
import conjgrad

import json
import numpy as np
import os
import time
import traceback

# Note that data will still be downloaded in ConvNet/data
PATHS = {'data': os.path.join('data'),
         'models': os.path.join('models')
        }

# SIZE_AFTER_POOL_TWICE = 1024
SIZE_AFTER_POOL_TWICE = 784

class ConvNet(nn.Module):
    def __init__(self) -> None:
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(IN_CHANNEL, 6, (3, 3), (1, 1), (1, 1))
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, (3, 3), (1, 1), (1, 1))
        self.fc1 = nn.Linear(SIZE_AFTER_POOL_TWICE, 120)
        self.fc2 = nn.Linear(120, 10)

    def forward(self, x):
        x = self.pool(Functional.relu(self.conv1(x)))
        x = self.pool(Functional.relu(self.conv2(x)))
        # Flatten the layer before connecting
        # Error caused by the tensor size drop from
        # 16*32*32 = 16384
        # 16384 / 4 / 4 = 1024
        x = x.view(-1, SIZE_AFTER_POOL_TWICE)
        x = Functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Utils funcitons
# Saving the losses data to file
def get_model_path(key, epoch, param_tuple):
    global PATHS
    param_name = '-'.join(str(param) for param in param_tuple)
    return os.path.join(PATHS['models'],
                        key,
                        param_name,
                        f'{key}-E{epoch}-{param_name}.pth')

def save_to_json(optimizers, key, param_tuple):
    global PATHS
    list_obj = {'time': optimizers[key]['time'],
                'validation_loss': optimizers[key]['loss'],
                'testing_loss': optimizers[key]['testing_loss']
               }
    param_name = '-'.join(str(param) for param in param_tuple)

    result_path = os.path.join(PATHS['models'], key, 'result')
    if not os.path.exists(result_path):
        os.mkdir(result_path)
    file_name = os.path.join(result_path, f'{key}-E{optimizers[key]["n_epochs"]-1}-{param_name}.json')

    # Save the array as JSON
    with open(file_name, 'w') as file:
        json.dump(list_obj, file)

def save_to_model(optimizers, key, path):
    if not os.path.exists(path):
        os.mkdir(path)
    try:
        torch.save(optimizers[key]['model'], os.path.join(path, f'{key}-E{optimizers[key]["n_epochs"]}.pth'))
    except:
        print(f'Failed to save the {key}-E{optimizers[key]["n_epochs"]}.pth')

def save_to_model_per_epoch(optimizers, key, epoch, param_tuple):
    global PATHS
    if optimizers[key]['params'] != None:
        pass
    param_name = '-'.join(str(param) for param in param_tuple)
    path = os.path.join(PATHS['models'], key)
    if not os.path.exists(path):
        os.mkdir(path)
    path = os.path.join(path, param_name)
    if not os.path.exists(path):
        os.mkdir(path)
    try:
        torch.save(optimizers[key]['model'], get_model_path(key, epoch, param_tuple))
    except:
        print(f'Failed to save the {key}-E{optimizers[key]["n_epochs"]}.pth')
        traceback.print_exc()

def load_model(key, epoch, param_tuple):
    global PATHS
    model_path = get_model_path(key, epoch, param_tuple)
    # Don't rerun if the model already exist
    if os.path.exists(model_path) == False:
        print(f'{model_path} does not exist')
        return -1
    return torch.load(model_path)

# Continuing with defining function
# Splits into different cells to avoid concantinate error when running

def train_for_n_epochs(param_tuple, key, model, optimizers_dict, trainloader):
# Reinitialise the model parameters to be saved --------------------------------
    optimizers[key]['loss'] = []
    optimizers[key]['time'] = []
    running_loss = 0
    optimizer = optimizers[key]['optimizer'](model, param_tuple)
# Starts training --------------------------------------------------------------
    for epoch in range(optimizers_dict[key]['n_epochs']):
        start_time = time.time()
        print(f'Epoch #{epoch}, {param_tuple}')
        for i, (inputs, labels) in enumerate(trainloader['train']):
            def closure():
                outputs = model(inputs)
                optimizer.zero_grad()
                loss = criteria(outputs, labels)
                loss.backward()
                return loss
            loss = optimizer.step(closure)
            running_loss += loss.item()

    # Print, append the loss value and reset the running loss ------------------
            if i % printfreq == printfreq-1:
                optimizers_dict[key]['loss'].append(running_loss/printfreq)
                print(f'_{running_loss/printfreq}')
                running_loss = 0
        optimizers_dict[key]["time"].append(time.time() - start_time) # In second

        print(f' {key}\'s loss == {optimizers_dict[key]["loss"][-1]}')
        optimizers_dict[key]['model'] = model

        save_to_model_per_epoch(optimizers_dict, key, epoch, param_tuple)
# End of training, save the data to json ---------------------------------------
    save_to_json(optimizers, key, param_tuple)

# Continuing with defining function
# Splits into different cells to avoid concantinate error when running

def testing_evaluation(key, n_epochs, param_tuple, trainloader):
    loss_arr = np.zeros(n_epochs)
    n_correct = 0
    n_samples = 0
    printfreq = len(trainloader['test']) * 0.1

    with torch.no_grad():
        print(f'Starts Testing now, key={key}, params={param_tuple}, n_epochs={n_epochs}')
        for j in range(n_epochs):
            try:
                for i, (image, label) in enumerate(trainloader['test']):
                    if i % printfreq == printfreq -1:
                        print('.', end='')
                    image.reshape(-1)
                    label = label.reshape(-1)
                    model = load_model(key, j, param_tuple)
                    try:
                        outputs = model(image)
                    except:
                        traceback.print_exc()
                        continue
                    # value, index
                    _, predictions = torch.max(outputs, 1)
                    n_samples += label.shape[0]
                    n_correct += (predictions == label).sum().item()

                acc = 100.0 * n_correct / n_samples
                print(f' Accuracy is {acc}%')
                loss_arr[j] = acc
            except KeyboardInterrupt:
                break
            except:
                print(f'{key} {j}\'s model failed to train.')
                traceback.print_exc()
    return list(loss_arr)

# Define the hyperparameters for training the data
BATCH_SIZE = 4
IN_CHANNEL = 1

# Random seed for reproducibility
seed = 42
torch.manual_seed(seed)

transform_3D = Transforms.Compose([Transforms.ToTensor(),
                                   Transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

transform_1D = Transforms.Compose([Transforms.ToTensor(),
                                   Transforms.Normalize((0.5,), (0.5,))])

dataset = {#'CIFAR10': {'train': torchvision.datasets.CIFAR10(root=PATHS['data'],
                    #                                          train=True,
                    #                                          download=True,
                    #                                          transform=transform_3D),
                    #    'test':  torchvision.datasets.CIFAR10(root=PATHS['data'],
                    #                                          train=False,
                    #                                          download=True,
                    #                                          transform=transform_3D)
                    #   },
           'MNIST': {'train': torchvision.datasets.MNIST(root=PATHS['data'],
                                                              train=True,
                                                              download=True,
                                                              transform=transform_1D),
                     'test': torchvision.datasets.MNIST(root=PATHS['data'],
                                                              train=False,
                                                              download=True,
                                                              transform=transform_1D)
                    }
          }

trainloader = dict(train=torch.utils.data.DataLoader(dataset['MNIST']['train'], batch_size=BATCH_SIZE,
                                                     shuffle=True),
                   test=torch.utils.data.DataLoader(dataset['MNIST']['test'], batch_size=BATCH_SIZE,
                                                     shuffle=False))

len(dataset['MNIST']['test'])

import importlib
importlib.reload(conjgrad)

# Initialise the loss function and optimizer

def SGD_init(model, params: tuple):
    _lr = params[0]
    return torch.optim.SGD(model.parameters(), lr=_lr)

def Adam_init(model, params: tuple):
    _lr = params[0]
    return torch.optim.Adam(model.parameters(),
                            lr=_lr,
                            betas=(0.9,0.999),
                            eps=1e-08,
                            weight_decay=0,
                            amsgrad=False)

def RMSprop_init(model, params: tuple):
    _lr = params[0]
    return torch.optim.RMSprop(model.parameters(), lr=_lr)

def Adadelta_init(model, params: tuple):
    _lr = params[0]
    return torch.optim.Adadelta(model.parameters(), lr=_lr)

def lbfgs_init(model, params: tuple):
    _lr = params[0]
    try:
        _max_iter = params[1]
    except IndexError:
        _max_iter = 5
    return torch.optim.LBFGS(model.parameters())#, lr=_lr, max_iter=_max_iter)

def cg_init(model, paras: tuple):
    return conjgrad.ConjGrad(model.parameters())

optimizers = {'SGD': {'optimizer': SGD_init,
                      'loss': [],
                      'time': [],
                      'testing_loss': [],
                      'n_epochs': 15,
                      'params': [(1e-4,), (1e-3,), (0.01,), (0.1,)],
                      'retrain': False
                      },
              'Adam': {'optimizer': Adam_init,
                      'loss': [],
                      'time': [],
                      'testing_loss': [],
                      'n_epochs': 15,
                      'params': [(1e-5,), (1e-4,), (1e-3,), (0.01,), (0.1,)],
                      'retrain': False
                      },
              'RMSprop': {'optimizer': RMSprop_init,
                          'loss': [],
                          'time': [],
                          'testing_loss': [],
                          'params': [(1e-5,), (1e-4,), (1e-3,), (0.01,), (0.1,)],
                          'n_epochs': 15,
                          'retrain': False
                         },
              'Adadelta': {'optimizer': Adadelta_init,
                          'loss': [],
                          'time': [],
                          'testing_loss': [],
                          'params': [(1e-4,), (1e-3,), (0.01,), (0.1,), (1,)],
                          'n_epochs': 15,
                           'retrain': False
                         },
              'lbfgs': {'optimizer': lbfgs_init,
                        'loss': [],
                        'time': [],
                        'testing_loss': [],
                        # 0.01 and 0.1 does not produce loss result at all
                        'params': [# (1e-5, 10), (1e-4, 10), (1e-3, 10),
                                   # (1e-5, 20), (1e-4, 20), (1e-3, 20)],
                                   (1,)],
                        'n_epochs': 15,
                        'retrain': False
                       },
              'cg': {'optimizer': cg_init,
                        'loss': [],
                        'time': [],
                        'testing_loss': [],
                        # 0.01 and 0.1 does not produce loss result at all
                        'params': [0.1],
                        'n_epochs': 10,
                        'retrain': True
                       }
             }

# The mode is in always save mode
# Pending to add clear array function before starts training

N_TOTAL_STEPS = len(trainloader['train'])
printfreq = int(N_TOTAL_STEPS * 0.1)

criteria = nn.CrossEntropyLoss()

for key in optimizers:
    try:
        print(f'{key}, n_epochs={optimizers[key]["n_epochs"]}, retrain={optimizers[key]["retrain"]}')
# Reinstantiate model everytime a new training started -------------------------
        model = ConvNet()
        model_path = os.path.join(PATHS['models'],
                                  key,
                                  f'{key}-E{optimizers[key]["n_epochs"]-1}.pth')
# Don't rerun if the model already exist ---------------------------------------
        if optimizers[key]['retrain'] != True:
            if os.path.exists(model_path) == False:
                print(f'{model_path} doesn\'t exist, not loaded in', end='\n\n')
                continue
            print(f'Skipping {key} because {model_path} exist')
            if 'model' not in optimizers[key] or optimizers[key]['model'] == None:
                optimizers[key]['model'] = torch.load(model_path)
                print(f'{model_path} is loaded into the {key}\'s model',
                      end='\n\n')
            continue

# Run for all different params -------------------------------------------------
# Run for n epochs -------------------------------------------------------------
        if optimizers[key]['params'] != None:
            for param_tuple in optimizers[key]['params']:
                train_for_n_epochs(param_tuple,
                                   key,
                                   model,
                                   optimizers,
                                   trainloader)
# Test the data after training -------------------------------------------------
                testing_loss = testing_evaluation(key=key,
                                                  n_epochs=optimizers[key]['n_epochs'],
                                                  param_tuple=param_tuple,
                                                  trainloader=trainloader)
                param_name = '-'.join(str(param) for param in param_tuple)
                path = os.path.join(PATHS['models'], key, 'result',
                                    f'{key}-E{optimizers[key]["n_epochs"]-1}-{param_name}.json')
                with open(path) as file:
                    json_data = json.load(file)
                optimizers[key]['time'] = json_data['time']
                optimizers[key]['loss'] = json_data['validation_loss']
                optimizers[key]['testing_loss'] = testing_loss
                # Save the file again to json ----------------------------------
                save_to_json(key=key,
                         optimizers=optimizers,
                         param_tuple=param_tuple)

        else:
            optimizer = optimizers[key]['optimizer'](model)
            train_for_n_epochs(optimizer, optimizers, key, model, trainloader)

    except KeyboardInterrupt:
        print("Keyboard Interrupted")
        break

    except:
        print(f'Error in training {key}')
        traceback.print_exc()
        continue
